{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capsule_Network_Project_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZvdAUbRrX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZatW3jZwR-38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "096de3ee-1295-4449-c481-88e509c26a7a"
      },
      "source": [
        "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available? True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTYBpe-9SA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "066be8f6-45b0-4d35-aa48-e2c31ad3c694"
      },
      "source": [
        "drive.mount(\"/content/drive\",True)\n",
        "root_dir = \"/content/drive/My Drive/SB3/\""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rldL-WKSBuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ColorJitter(),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTAga3c5SGos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e07d7c20-d279-42cf-ae35-50fa030e2e16"
      },
      "source": [
        "train_dataset = ImageFolder(os.path.join(root_dir, \"train\"), transform=train_transform)\n",
        "test_dataset = ImageFolder(os.path.join(root_dir, \"test\"), transform=test_transform)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA59yhfx3rj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7a35f6dd-0eaa-4519-edbf-72b3e342225f"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmvwRJDU_-UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff7d6792-2b35-4786-b824-32672034231d"
      },
      "source": [
        "counts = torch.zeros(num_classes)\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "  counts[train_dataset[i][1]] += 1.0\n",
        "\n",
        "print(counts)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([500.,  28.,  38., 116.,  17.,  19., 339.,  52.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lJmDj1aHfa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = len(train_dataset) / counts\n",
        "weights = weights.to(dev)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIXTmrPp0C7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d9451e56-a1ef-4b5d-a7f6-67172d2336d2"
      },
      "source": [
        "x = train_dataset[0][0]\n",
        "print(x.max())\n",
        "print(x.min())"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9961)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P10rGKk8SIW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = len(train_dataset)\n",
        "train_idx = list(range(num_train))\n",
        "random.shuffle(train_idx)\n",
        "val_frac = 0.2\n",
        "num_val = int(num_train*val_frac)\n",
        "num_train = num_train - num_val\n",
        "val_idx = train_idx[num_train:]\n",
        "train_idx = train_idx[:num_train]\n",
        "val_dataset = Subset(train_dataset,val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9SIeKL_SMm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset,   batch_size=4, num_workers=4, shuffle=False)\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\" : val_loader,\n",
        "    \"test\": test_loader\n",
        "}"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgR4FQ2tU2jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, dim=-1):\n",
        "\t'''\n",
        "\t\"Squashing\" non-linearity that shrunks short vectors to almost zero length and long vectors to a length slightly below 1\n",
        "\tEq. (1): v_j = ||s_j||^2 / (1 + ||s_j||^2) * s_j / ||s_j||\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ts: \tVector before activation\n",
        "\t\tdim:\tDimension along which to calculate the norm\n",
        "\t\n",
        "\tReturns:\n",
        "\t\tSquashed vector\n",
        "\t'''\n",
        "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
        "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q1JXyMoSQKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, vector_length, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Initialize the layer.\n",
        "    Args:\n",
        "      in_channels: \tNumber of input channels.\n",
        "      out_channels: \tNumber of output channels.\n",
        "      vector_length:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vector_length = vector_length\n",
        "    self.num_caps_channels = int(out_channels / vector_length)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), self.num_caps_channels, x.size(2), x.size(3), self.vector_length)\n",
        "    x = x.view(x.size(0), -1, self.vector_length)\n",
        "    return squash(x)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6uiDtWVSmUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoutingCapsules(nn.Module):\n",
        "  def __init__(self, in_vector_length, num_in_caps, num_out_caps, out_vector_length, num_routing):\n",
        "    '''\n",
        "\t\tInitialize the layer.\n",
        "\t\tArgs:\n",
        "\t\t\tin_vector_length: \t\tDimensionality (i.e. length) of each capsule vector.\n",
        "\t\t\tnum_in_caps: \t\tNumber of input capsules if digits layer.\n",
        "\t\t\tnum_out_caps: \t\tNumber of capsules in the capsule layer\n",
        "\t\t\tout_vector_length: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\t\t\tnum_routing:\tNumber of iterations during routing algorithm\t\t\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.in_vector_length = in_vector_length\n",
        "    self.num_in_caps = num_in_caps\n",
        "    self.num_out_caps = num_out_caps\n",
        "    self.out_vector_length = out_vector_length\n",
        "    self.num_routing = num_routing\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(1, num_out_caps, num_in_caps, out_vector_length, in_vector_length ) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    # (batch_size, num_in_caps, in_vector_length) -> (batch_size, 1, num_in_caps, in_vector_length, 1)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    #\n",
        "    # W @ x =\n",
        "    # (1, num_output_caps, num_in_caps, out_vector_length, in_vector_length) @ (batch_size, 1, num_in_caps, in_vector_length, 1) =\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    # detach u_hat during routing iterations to prevent gradients from flowing\n",
        "    temp_u_hat = u_hat.detach()\n",
        "\n",
        "    '''\n",
        "    Procedure 1: Routing algorithm\n",
        "    '''\n",
        "    b = torch.zeros(batch_size, self.num_out_caps, self.num_in_caps, 1).to(dev)\n",
        "\n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) -> Softmax along num_out_caps\n",
        "      c = F.softmax(b, dim=1)\n",
        "\n",
        "      # element-wise multiplication\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) * (batch_size, num_in_caps, num_out_caps, out_vector_length) ->\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) sum across num_in_caps ->\n",
        "      # (batch_size, num_out_caps, out_vector_length)\n",
        "      s = (c * temp_u_hat).sum(dim=2)\n",
        "      # apply \"squashing\" non-linearity along dim_caps\n",
        "      v = squash(s)\n",
        "      # dot product agreement between the current output vj and the prediction uj|i\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) @ (batch_size, num_out_caps, out_vector_length, 1)\n",
        "      # -> (batch_size, num_out_caps, num_in_caps, 1)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "\n",
        "    # last iteration is done on the original u_hat, without the routing weights update\n",
        "    c = F.softmax(b, dim=1)\n",
        "    s = (c * u_hat).sum(dim=2)\n",
        "    # apply \"squashing\" non-linearity along dim_caps\n",
        "    v = squash(s)\n",
        "\n",
        "    return v"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZmQKyYPSQST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, num_classes, vector_length):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(num_classes*vector_length, 64),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.reconstruction_layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(1, 128, kernel_size=5, padding=2, stride=11),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2, stride=4, output_padding=(1,1)),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(64, 3, kernel_size=5, padding=7, stride=1),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.fc_layer(x)\n",
        "      x = x.view(x.size(0), 1, 8, 8)\n",
        "      x = self.reconstruction_layers(x)\n",
        "      return x"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWxbVp1SRBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=5, padding=2, stride=8),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.primary_caps = PrimaryCapsules(in_channels=64, out_channels=128, vector_length=8, kernel_size=5, padding=2, stride=1)\n",
        "      self.digit_caps = nn.Sequential(\n",
        "        RoutingCapsules(in_vector_length=8, num_in_caps=25600, num_out_caps=num_classes, out_vector_length=16, num_routing=3),\n",
        "        RoutingCapsules(in_vector_length=16, num_in_caps=num_classes, num_out_caps=num_classes, out_vector_length=16, num_routing=3)\n",
        "      )\n",
        "      self.reconstruction_layer = Reconstruction(num_classes=num_classes, vector_length=16)\n",
        "    \n",
        "    '''\n",
        "    def capsule_average_pooling(self, x):\n",
        "      height = x.size(3)\n",
        "      width = x.size(4)\n",
        "      x = x.sum(dim=4).sum(dim=3)\n",
        "      x = x / (height * width)\n",
        "      return x\n",
        "    '''\n",
        "    \n",
        "    def score(self, x):\n",
        "      return torch.sqrt((x ** 2).sum(dim=2))\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.conv_layer(x)\n",
        "      x = self.primary_caps(x)\n",
        "      x = self.digit_caps(x)\n",
        "      scores = self.score(x)\n",
        "      x = x.view(x.size(0), x.size(1) * x.size(2))\n",
        "      reconstructions = self.reconstruction_layer(x)\n",
        "      return scores, reconstructions"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9EcOQTmYI3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CapsNet(num_classes=num_classes)\n",
        "model = model.to(dev)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghnqtQeaxam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8cbaaafc-7615-4d3c-ef15-f574950ad29d"
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "#print(batch.size())\n",
        "scores, reconstructions = model(batch)\n",
        "print(scores.size())\n",
        "print(reconstructions.size())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 3, 320, 320])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXC9UxPY0gnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "#print(batch.size())\n",
        "out = model(batch)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_iemIRmlwd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, dev, lr=0.001):\n",
        "    try:\n",
        "        # Create model\n",
        "        model = CapsNet(num_classes=num_classes)\n",
        "        model = model.to(dev)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        # Initialize history\n",
        "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        # Process each epoch\n",
        "        for epoch in range(epochs):\n",
        "            # Initialize epoch variables\n",
        "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            \n",
        "            # Process each split\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                  model.train()\n",
        "                else:\n",
        "                  model.eval()\n",
        "                # Process each batch\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    # Move to CUDA\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    # Reset gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # Compute output\n",
        "                    pred, reconstructions = model(input)\n",
        "                    score_loss = F.cross_entropy(pred, labels, weight=weights)\n",
        "                    reconstruction_loss = F.mse_loss(input, reconstructions)\n",
        "                    loss = score_loss + reconstruction_loss\n",
        "                    # Update loss\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    # Check parameter update\n",
        "                    if split == \"train\":\n",
        "                        # Compute gradients\n",
        "                        loss.backward()\n",
        "                        # Optimize\n",
        "                        optimizer.step()\n",
        "                    # Compute accuracy\n",
        "                    _,pred_labels = pred.max(1)\n",
        "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
        "                    # Update accuracy\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "            # Compute epoch loss/accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            # Update history\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "            # Print info\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
        "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"VL={epoch_loss['val']:.4f},\",\n",
        "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
        "                  f\"TeA={epoch_accuracy['test']:.4f},\"\n",
        "                )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        # Plot loss\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # Plot accuracy\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcTyH7emXjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb027145-1957-4cb4-dbd0-50bd029e938a"
      },
      "source": [
        "train(100, dev, lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: TrL=2.0871, TrA=0.4200, VL=2.0186, VA=0.4509, TeL=1.8672, TeA=0.4534,\n",
            "Epoch 2: TrL=2.0579, TrA=0.4493, VL=2.0434, VA=0.4509, TeL=1.8886, TeA=0.4534,\n",
            "Epoch 3: TrL=1.9971, TrA=0.4493, VL=1.9907, VA=0.4375, TeL=1.8102, TeA=0.4492,\n",
            "Epoch 4: TrL=2.0089, TrA=0.4234, VL=1.9575, VA=0.5000, TeL=1.8028, TeA=0.5148,\n",
            "Epoch 5: TrL=1.9971, TrA=0.4505, VL=1.9762, VA=0.4509, TeL=1.8141, TeA=0.4534,\n",
            "Epoch 6: TrL=1.9887, TrA=0.4493, VL=1.9336, VA=0.4509, TeL=1.7591, TeA=0.4534,\n",
            "Epoch 7: TrL=1.9609, TrA=0.4493, VL=1.9235, VA=0.4509, TeL=1.7519, TeA=0.4534,\n",
            "Epoch 8: TrL=1.9663, TrA=0.4493, VL=1.9733, VA=0.4509, TeL=1.8273, TeA=0.4534,\n",
            "Epoch 9: TrL=1.9300, TrA=0.4493, VL=2.0337, VA=0.4509, TeL=1.8032, TeA=0.4534,\n",
            "Epoch 10: TrL=1.9318, TrA=0.4493, VL=1.9244, VA=0.4509, TeL=1.7283, TeA=0.4534,\n",
            "Epoch 11: TrL=1.9429, TrA=0.4606, VL=1.9631, VA=0.4866, TeL=1.7992, TeA=0.5212,\n",
            "Epoch 12: TrL=1.9256, TrA=0.4550, VL=1.9366, VA=0.4955, TeL=1.7682, TeA=0.5148,\n",
            "Epoch 13: TrL=1.9177, TrA=0.4583, VL=1.8934, VA=0.4554, TeL=1.7375, TeA=0.4894,\n",
            "Epoch 14: TrL=1.9150, TrA=0.4831, VL=1.8932, VA=0.5000, TeL=1.7543, TeA=0.5191,\n",
            "Epoch 15: TrL=1.8615, TrA=0.4167, VL=1.8696, VA=0.5000, TeL=1.6986, TeA=0.5042,\n",
            "Epoch 16: TrL=1.8591, TrA=0.4910, VL=1.8662, VA=0.4598, TeL=1.7225, TeA=0.4682,\n",
            "Epoch 17: TrL=1.8227, TrA=0.4820, VL=1.8688, VA=0.5000, TeL=1.7329, TeA=0.5148,\n",
            "Epoch 18: TrL=1.8240, TrA=0.4887, VL=1.8396, VA=0.5000, TeL=1.7188, TeA=0.5148,\n",
            "Epoch 19: TrL=1.7961, TrA=0.4899, VL=1.8655, VA=0.5000, TeL=1.7042, TeA=0.5148,\n",
            "Epoch 20: TrL=1.7619, TrA=0.4887, VL=1.8320, VA=0.5000, TeL=1.7035, TeA=0.5148,\n",
            "Epoch 21: TrL=1.7340, TrA=0.4910, VL=1.9007, VA=0.4866, TeL=1.6981, TeA=0.4958,\n",
            "Epoch 22: TrL=1.7451, TrA=0.4820, VL=1.8648, VA=0.4777, TeL=1.6934, TeA=0.4831,\n",
            "Epoch 23: TrL=1.6932, TrA=0.4786, VL=1.8074, VA=0.5000, TeL=1.6822, TeA=0.4831,\n",
            "Epoch 24: TrL=1.6684, TrA=0.4797, VL=1.8711, VA=0.4911, TeL=1.6846, TeA=0.4725,\n",
            "Epoch 25: TrL=1.6403, TrA=0.4797, VL=1.8639, VA=0.4643, TeL=1.6735, TeA=0.4682,\n",
            "Epoch 26: TrL=1.6361, TrA=0.4696, VL=1.8419, VA=0.4911, TeL=1.6569, TeA=0.4894,\n",
            "Epoch 27: TrL=1.6206, TrA=0.4899, VL=1.8323, VA=0.4732, TeL=1.6778, TeA=0.4703,\n",
            "Epoch 28: TrL=1.5999, TrA=0.4842, VL=1.8222, VA=0.4688, TeL=1.6429, TeA=0.4809,\n",
            "Epoch 29: TrL=1.5761, TrA=0.4977, VL=1.7868, VA=0.5000, TeL=1.6416, TeA=0.5169,\n",
            "Epoch 30: TrL=1.5698, TrA=0.5045, VL=1.8405, VA=0.4911, TeL=1.6908, TeA=0.5000,\n",
            "Epoch 31: TrL=1.5763, TrA=0.4977, VL=1.8603, VA=0.5134, TeL=1.6554, TeA=0.5127,\n",
            "Epoch 32: TrL=1.5622, TrA=0.5101, VL=1.7966, VA=0.5268, TeL=1.6361, TeA=0.5318,\n",
            "Epoch 33: TrL=1.5590, TrA=0.5180, VL=1.8075, VA=0.5089, TeL=1.6348, TeA=0.5212,\n",
            "Epoch 34: TrL=1.5607, TrA=0.5383, VL=1.8061, VA=0.4866, TeL=1.6493, TeA=0.5445,\n",
            "Epoch 35: TrL=1.5423, TrA=0.5800, VL=1.8036, VA=0.4732, TeL=1.6341, TeA=0.5699,\n",
            "Epoch 36: TrL=1.5271, TrA=0.6678, VL=1.8258, VA=0.5000, TeL=1.6508, TeA=0.5636,\n",
            "Epoch 37: TrL=1.5122, TrA=0.6847, VL=1.7843, VA=0.4955, TeL=1.6380, TeA=0.5431,\n",
            "Epoch 38: TrL=1.4750, TrA=0.7038, VL=1.7801, VA=0.5223, TeL=1.6423, TeA=0.5600,\n",
            "Epoch 39: TrL=1.4813, TrA=0.6948, VL=1.7883, VA=0.5223, TeL=1.6470, TeA=0.5346,\n",
            "Epoch 40: TrL=1.4639, TrA=0.7173, VL=1.7969, VA=0.4643, TeL=1.6516, TeA=0.4965,\n",
            "Epoch 41: TrL=1.4583, TrA=0.6836, VL=1.7922, VA=0.4464, TeL=1.6936, TeA=0.4661,\n",
            "Epoch 42: TrL=1.4560, TrA=0.7117, VL=1.7657, VA=0.5179, TeL=1.6376, TeA=0.5692,\n",
            "Epoch 43: TrL=1.4506, TrA=0.7106, VL=1.7392, VA=0.4866, TeL=1.6390, TeA=0.5572,\n",
            "Epoch 44: TrL=1.4314, TrA=0.7297, VL=1.7551, VA=0.4821, TeL=1.6603, TeA=0.5134,\n",
            "Epoch 45: TrL=1.4283, TrA=0.7523, VL=1.7574, VA=0.4955, TeL=1.6450, TeA=0.5636,\n",
            "Epoch 46: TrL=1.4301, TrA=0.7387, VL=1.8028, VA=0.4688, TeL=1.6444, TeA=0.5840,\n",
            "Epoch 47: TrL=1.4267, TrA=0.7264, VL=1.7802, VA=0.5357, TeL=1.6301, TeA=0.5777,\n",
            "Epoch 48: TrL=1.4117, TrA=0.7534, VL=1.7449, VA=0.5045, TeL=1.6461, TeA=0.5629,\n",
            "Epoch 49: TrL=1.4077, TrA=0.7601, VL=1.7673, VA=0.5357, TeL=1.6280, TeA=0.5840,\n",
            "Epoch 50: TrL=1.4022, TrA=0.7759, VL=1.7887, VA=0.5089, TeL=1.6358, TeA=0.5629,\n",
            "Epoch 51: TrL=1.4028, TrA=0.7725, VL=1.7388, VA=0.5000, TeL=1.6509, TeA=0.5734,\n",
            "Epoch 52: TrL=1.3891, TrA=0.8029, VL=1.7930, VA=0.5179, TeL=1.6532, TeA=0.5812,\n",
            "Epoch 53: TrL=1.3848, TrA=0.7962, VL=1.7507, VA=0.5223, TeL=1.6457, TeA=0.5883,\n",
            "Epoch 54: TrL=1.3887, TrA=0.7905, VL=1.7384, VA=0.5268, TeL=1.6371, TeA=0.5777,\n",
            "Epoch 55: TrL=1.3814, TrA=0.7995, VL=1.7642, VA=0.5268, TeL=1.6359, TeA=0.6017,\n",
            "Epoch 56: TrL=1.3770, TrA=0.8097, VL=1.7650, VA=0.5179, TeL=1.6324, TeA=0.5911,\n",
            "Epoch 57: TrL=1.3891, TrA=0.8119, VL=1.7389, VA=0.5536, TeL=1.6231, TeA=0.5713,\n",
            "Epoch 58: TrL=1.3854, TrA=0.7995, VL=1.7833, VA=0.5312, TeL=1.6196, TeA=0.6095,\n",
            "Epoch 59: TrL=1.3833, TrA=0.8074, VL=1.7859, VA=0.5580, TeL=1.6850, TeA=0.5290,\n",
            "Epoch 60: TrL=1.3807, TrA=0.8142, VL=1.7563, VA=0.5625, TeL=1.6330, TeA=0.5734,\n",
            "Epoch 61: TrL=1.3703, TrA=0.8119, VL=1.7802, VA=0.5491, TeL=1.6178, TeA=0.5812,\n",
            "Epoch 62: TrL=1.3724, TrA=0.8131, VL=1.7651, VA=0.5357, TeL=1.6301, TeA=0.5819,\n",
            "Epoch 63: TrL=1.3622, TrA=0.8198, VL=1.7900, VA=0.5357, TeL=1.6289, TeA=0.6010,\n",
            "Epoch 64: TrL=1.3562, TrA=0.8345, VL=1.7405, VA=0.5446, TeL=1.6276, TeA=0.5720,\n",
            "Epoch 65: TrL=1.3571, TrA=0.8345, VL=1.7125, VA=0.5625, TeL=1.6306, TeA=0.5883,\n",
            "Epoch 66: TrL=1.3563, TrA=0.8435, VL=1.7749, VA=0.5625, TeL=1.6309, TeA=0.5918,\n",
            "Epoch 67: TrL=1.3566, TrA=0.8570, VL=1.7698, VA=0.5491, TeL=1.6383, TeA=0.5989,\n",
            "Epoch 68: TrL=1.3570, TrA=0.8649, VL=1.7586, VA=0.5625, TeL=1.6183, TeA=0.5946,\n",
            "Epoch 69: TrL=1.3536, TrA=0.8423, VL=1.7203, VA=0.5848, TeL=1.6299, TeA=0.6102,\n",
            "Epoch 70: TrL=1.3737, TrA=0.8345, VL=1.8120, VA=0.4732, TeL=1.6784, TeA=0.5318,\n",
            "Epoch 71: TrL=1.3654, TrA=0.8423, VL=1.7633, VA=0.5804, TeL=1.6443, TeA=0.5784,\n",
            "Epoch 72: TrL=1.3543, TrA=0.8592, VL=1.7528, VA=0.5938, TeL=1.6260, TeA=0.5968,\n",
            "Epoch 73: TrL=1.3512, TrA=0.8637, VL=1.7677, VA=0.5446, TeL=1.6298, TeA=0.5862,\n",
            "Epoch 74: TrL=1.3455, TrA=0.8705, VL=1.7611, VA=0.5670, TeL=1.6369, TeA=0.5756,\n",
            "Epoch 75: TrL=1.3654, TrA=0.8243, VL=1.7875, VA=0.5536, TeL=1.6469, TeA=0.6045,\n",
            "Epoch 76: TrL=1.3565, TrA=0.8649, VL=1.7323, VA=0.5536, TeL=1.6750, TeA=0.5551,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}