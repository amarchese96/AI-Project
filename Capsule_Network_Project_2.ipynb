{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capsule_Network_Project_2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzstYvr7w4pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import itertools as it"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2SRXbbuxY9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5eeeaa3c-44ac-41dd-d584-ec1308bf641f"
      },
      "source": [
        "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available? True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAZWiLvNyB17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "06d97e33-9e45-4caa-8001-85113e40a819"
      },
      "source": [
        "drive.mount(\"/content/drive\",True)\n",
        "root_dir = \"/content/drive/My Drive/SB3/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uqJLqOfyYX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, dim=-1):\n",
        "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
        "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE99mgZ3yfho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, vector_length, kernel_size, stride, padding):\n",
        "    super().__init__()\n",
        "    self.vector_length = vector_length\n",
        "    self.num_caps_channels = int(out_channels / vector_length)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), self.num_caps_channels, x.size(2), x.size(3), self.vector_length)\n",
        "    x = x.view(x.size(0), -1, self.vector_length)\n",
        "    return squash(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w81OQ4-tym1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoutingCapsules(nn.Module):\n",
        "  def __init__(self, in_vector_length, num_in_caps, num_out_caps, out_vector_length, num_routing):\n",
        "    super().__init__()\n",
        "    self.in_vector_length = in_vector_length\n",
        "    self.num_in_caps = num_in_caps\n",
        "    self.num_out_caps = num_out_caps\n",
        "    self.out_vector_length = out_vector_length\n",
        "    self.num_routing = num_routing\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(1, num_out_caps, num_in_caps, out_vector_length, in_vector_length ) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    # (batch_size, num_in_caps, in_vector_length) -> (batch_size, 1, num_in_caps, in_vector_length, 1)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    #\n",
        "    # W @ x =\n",
        "    # (1, num_output_caps, num_in_caps, out_vector_length, in_vector_length) @ (batch_size, 1, num_in_caps, in_vector_length, 1) =\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    # detach u_hat during routing iterations to prevent gradients from flowing\n",
        "    temp_u_hat = u_hat.detach()\n",
        "\n",
        "    '''\n",
        "    Procedure 1: Routing algorithm\n",
        "    '''\n",
        "    b = torch.zeros(batch_size, self.num_out_caps, self.num_in_caps, 1).to(dev)\n",
        "\n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) -> Softmax along num_out_caps\n",
        "      c = F.softmax(b, dim=1)\n",
        "\n",
        "      # element-wise multiplication\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) * (batch_size, num_in_caps, num_out_caps, out_vector_length) ->\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) sum across num_in_caps ->\n",
        "      # (batch_size, num_out_caps, out_vector_length)\n",
        "      s = (c * temp_u_hat).sum(dim=2)\n",
        "      # apply \"squashing\" non-linearity along dim_caps\n",
        "      v = squash(s)\n",
        "      # dot product agreement between the current output vj and the prediction uj|i\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) @ (batch_size, num_out_caps, out_vector_length, 1)\n",
        "      # -> (batch_size, num_out_caps, num_in_caps, 1)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "\n",
        "    # last iteration is done on the original u_hat, without the routing weights update\n",
        "    c = F.softmax(b, dim=1)\n",
        "    s = (c * u_hat).sum(dim=2)\n",
        "    # apply \"squashing\" non-linearity along dim_caps\n",
        "    v = squash(s)\n",
        "\n",
        "    return v"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k1WJN3cyt9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, num_classes, vector_length):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(num_classes*vector_length, 64),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.reconstruction_layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(1, 128, kernel_size=5, padding=2, stride=5),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2, stride=3),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ConvTranspose2d(64, 32, kernel_size=5, padding=2, stride=3),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Conv2d(32, 3, kernel_size=5, padding=4, stride=1),\n",
        "          nn.Sigmoid()\n",
        "    )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.fc_layer(x)\n",
        "      x = x.view(x.size(0), 1, 8, 8)\n",
        "      x = self.reconstruction_layers(x)\n",
        "      return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF2zJ9xjyzjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=5, padding=2, stride=4),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.MaxPool2d(kernel_size=5, padding=2, stride=2)\n",
        "      )\n",
        "      self.primary_caps = PrimaryCapsules(in_channels=64, out_channels=128, vector_length=8, kernel_size=5, padding=2, stride=1)\n",
        "      self.digit_caps = nn.Sequential(\n",
        "        RoutingCapsules(in_vector_length=8, num_in_caps=25600, num_out_caps=20, out_vector_length=16, num_routing=3),\n",
        "        RoutingCapsules(in_vector_length=16, num_in_caps=20, num_out_caps=num_classes, out_vector_length=16, num_routing=3)\n",
        "      )\n",
        "      self.reconstruction_layer = Reconstruction(num_classes=num_classes, vector_length=16)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.conv_layer(x)\n",
        "      x = self.primary_caps(x)\n",
        "      x = self.digit_caps(x)\n",
        "      scores = torch.norm(x,dim=-1)\n",
        "      x = x.view(x.size(0), x.size(1) * x.size(2))\n",
        "      reconstructions = self.reconstruction_layer(x)\n",
        "      return scores, reconstructions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9A8ExHxy5XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def margin_loss(pred, labels, num_classes, m_plus=0.9, m_minus=0.1, loss_lambda=0.5):\n",
        "  one_hot_labels = F.one_hot(labels,num_classes = num_classes)\n",
        "  loss = one_hot_labels * F.relu(m_plus - pred)**2 + loss_lambda * (1 - one_hot_labels) * F.relu(pred - m_minus)**2\n",
        "  loss = loss.sum(dim=1)\n",
        "  return loss.mean()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SybeYa0pzBuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epochs, dev, num_classes, loaders, lr=0.001, reconstruction_loss_scale=5e-1):\n",
        "    try:\n",
        "        # Create model\n",
        "        model = CapsNet(num_classes=num_classes)\n",
        "        model = model.to(dev)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        # Initialize history\n",
        "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        # Initialize best validation accuracy and test accuracy at best validation accuracy\n",
        "        best_val_accuracy = 0.0\n",
        "        test_accuracy_at_best_val = 0.0\n",
        "        best_val_per_class_accuracy = torch.zeros(num_classes).to(dev)\n",
        "        test_per_class_accuracy_at_best_val = torch.zeros(num_classes).to(dev)\n",
        "        batch_confusion_matrix = torch.zeros(num_classes, num_classes , dtype=torch.int64).to(dev)\n",
        "        # Process each epoch\n",
        "        for epoch in range(epochs):\n",
        "            # Initialize epoch variables\n",
        "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            confusion_matrix = {\"train\": torch.zeros(num_classes, num_classes , dtype=torch.int64).to(dev), \"val\": torch.zeros(num_classes, num_classes , dtype=torch.int64).to(dev), \"test\": torch.zeros(num_classes, num_classes , dtype=torch.int64).to(dev)}\n",
        "            # Process each split\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                  model.train()\n",
        "                else:\n",
        "                  model.eval()\n",
        "                # Process each batch\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    # Move to CUDA\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    # Reset gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # Compute output\n",
        "                    pred, reconstructions = model(input)\n",
        "                    #score_loss = F.cross_entropy(pred, labels, weight=weights)\n",
        "                    score_loss = margin_loss(pred, labels, num_classes, m_plus=0.9, m_minus=0.1, loss_lambda=0.5)\n",
        "                    reconstruction_loss = F.mse_loss(input, reconstructions) * reconstruction_loss_scale\n",
        "                    loss = score_loss + reconstruction_loss\n",
        "                    # Update loss\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    # Check parameter update\n",
        "                    if split == \"train\":\n",
        "                        # Compute gradients\n",
        "                        loss.backward()\n",
        "                        # Optimize\n",
        "                        optimizer.step()\n",
        "                    # Compute accuracy\n",
        "                    _,pred_labels = pred.max(1)\n",
        "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
        "                    # Update accuracy\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "                    #Update Confusion Matrix\n",
        "                    for label, pred in zip(labels.view(-1), pred_labels.view(-1)):\n",
        "                      confusion_matrix[split][label, pred] += 1\n",
        "            # Compute epoch loss/accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            # Update history\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "            # Print info\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
        "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"VL={epoch_loss['val']:.4f},\",\n",
        "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
        "                  f\"TeA={epoch_accuracy['test']:.4f},\\n\"\n",
        "                )\n",
        "            # Check if we obtained the best validation accuracy\n",
        "            if best_val_accuracy == 0.0 or epoch_accuracy[\"val\"] > best_val_accuracy:\n",
        "              # Update best validation accuracy and test accuracy at best validation\n",
        "              best_val_accuracy = epoch_accuracy[\"val\"]\n",
        "              test_accuracy_at_best_val = epoch_accuracy[\"test\"]\n",
        "              best_val_per_class_accuracy = confusion_matrix['val'].diag()/confusion_matrix['val'].sum(1).double()\n",
        "              test_per_class_accuracy_at_best_val = confusion_matrix['test'].diag()/confusion_matrix['test'].sum(1).double()\n",
        "            #Print per-class accuracy\n",
        "            print(f\"Training per-class accuracy: {(confusion_matrix['train'].diag()/confusion_matrix['train'].sum(1).double()).tolist()}\")\n",
        "            print(f\"Validation per-class accuracy: {(confusion_matrix['val'].diag()/confusion_matrix['val'].sum(1).double()).tolist()}\")\n",
        "            print(f\"Test per-class accuracy: {(confusion_matrix['test'].diag()/confusion_matrix['test'].sum(1).double()).tolist()} \\n\")\n",
        "            print(f\"----------------------------------\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        # Plot loss\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # Plot accuracy\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        return(best_val_accuracy, test_accuracy_at_best_val, best_val_per_class_accuracy, test_per_class_accuracy_at_best_val)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROdSFZPL0Xbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WrapperDataset(Dataset):\n",
        "  def __init__(self, dataset, map_fn):\n",
        "    self.dataset = dataset\n",
        "    self.map = map_fn\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return (self.map(self.dataset[index][0]), self.dataset[index][1])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amuT30br0Sln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBvyhrzS0ILk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db1094b8-3403-4032-9a62-ad41219eb7ab"
      },
      "source": [
        "train_dataset = ImageFolder(os.path.join(root_dir, \"train\"))\n",
        "test_dataset = ImageFolder(os.path.join(root_dir, \"test\"), transform=test_transform)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC41vuVn1FLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1365b9ae-ae99-49db-8797-c00b79c4bf3d"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogzA6sor3p9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "af365c00-9f91-4e06-8720-683ecc5d75c4"
      },
      "source": [
        "val_frac = 0.2\n",
        "class_counts = [train_dataset.targets.count(i) for i in range(num_classes)]\n",
        "print(f\"Class counts {class_counts}\")\n",
        "train_counts = [x-int(x*val_frac) for x in class_counts]\n",
        "print(f\"Train counts {train_counts}\")\n",
        "val_counts = [int(x*val_frac) for x in class_counts]\n",
        "print(f\"Val counts {val_counts}\")\n",
        "test_counts = [test_dataset.targets.count(i) for i in range(num_classes)]\n",
        "print(f\"Test counts {test_counts}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class counts [500, 28, 38, 116, 17, 19, 339, 52]\n",
            "Train counts [400, 23, 31, 93, 14, 16, 272, 42]\n",
            "Val counts [100, 5, 7, 23, 3, 3, 67, 10]\n",
            "Test counts [214, 12, 16, 49, 6, 7, 145, 22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKBblhFm3L_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_dataset, test_dataset, class_idx, train_transform, test_transform, val_frac, num_classes, index):\n",
        "  if index < int(1 / val_frac) - 1:\n",
        "    val_idx = [x[index*int(len(x)*val_frac):index*int(len(x)*val_frac) + int(len(x)*val_frac)] for x in class_idx]\n",
        "    train_idx = [x[:index*int(len(x)*val_frac)] + x[index*int(len(x)*val_frac) + int(len(x)*val_frac):] for x in class_idx]\n",
        "  else:\n",
        "    val_idx = [x[index*int(len(x)*val_frac):] for x in class_idx]\n",
        "    train_idx = [x[:index*int(len(x)*val_frac)] for x in class_idx]\n",
        "  \n",
        "  train_idx = list(it.chain.from_iterable(train_idx))\n",
        "  val_idx = list(it.chain.from_iterable(val_idx))\n",
        "  \n",
        "  val_dataset = Subset(train_dataset,val_idx)\n",
        "  train_dataset = Subset(train_dataset, train_idx)\n",
        "\n",
        "  train_dataset = WrapperDataset(train_dataset,train_transform)\n",
        "  val_dataset = WrapperDataset(val_dataset,test_transform)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=4, num_workers=4, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset,   batch_size=4, num_workers=4, shuffle=False)\n",
        "  loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\" : val_loader,\n",
        "    \"test\": test_loader\n",
        "  }\n",
        "\n",
        "  return train_model(30, dev, num_classes, loaders, lr=0.001, reconstruction_loss_scale=5e-1)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vR1U6QsZarK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = len(train_dataset)\n",
        "idx = list(range(num_train))\n",
        "idx_iter = iter(idx)\n",
        "class_idx = [list(it.islice(idx_iter, x)) for x in class_counts]\n",
        "class_idx = [random.sample(x,len(x)) for x in class_idx]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaN_p3_T66rD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = int(1 / val_frac)\n",
        "sum_best_val_accuracy = 0.0\n",
        "sum_test_accuracy_at_best_val = 0.0\n",
        "sum_best_val_per_class_accuracy = torch.zeros(num_classes).to(dev)\n",
        "sum_test_per_class_accuracy_at_best_val = torch.zeros(num_classes).to(dev)\n",
        "for i in range(k):\n",
        "  best_val_accuracy, test_accuracy_at_best_val, best_val_per_class_accuracy, test_per_class_accuracy_at_best_val = train(train_dataset, test_dataset, class_idx, train_transform, test_transform, val_frac, num_classes, i)\n",
        "  sum_best_val_accuracy += best_val_accuracy\n",
        "  sum_test_accuracy_at_best_val += test_accuracy_at_best_val\n",
        "  sum_best_val_per_class_accuracy += best_val_per_class_accuracy\n",
        "  sum_test_per_class_accuracy_at_best_val += test_per_class_accuracy_at_best_val\n",
        "  print(\"--------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ricYeQkt8K3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e6ab391a-71e1-45e5-e264-5f2955b72dc1"
      },
      "source": [
        "# Print best validation accuracy and test accuracy at best validation \n",
        "print(f\"Mean best validation accuracy: {sum_best_val_accuracy / k}\")\n",
        "print(f\"Mean test accuracy at best validation: {sum_test_accuracy_at_best_val / k}\")\n",
        "print(f\"Mean best per-class validation accuracy: {(sum_best_val_per_class_accuracy / k).tolist()}\")\n",
        "print(f\"Mean test per-class accuracy at best validation: {(sum_test_per_class_accuracy_at_best_val / k).tolist()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean best validation accuracy: 0.7636363636363637\n",
            "Mean test accuracy at best validation: 0.7561793785310734\n",
            "Mean best per-class validation accuracy: [0.8224999904632568, 0.949999988079071, 0.5357142686843872, 0.695652186870575, 0.5833333134651184, 0.8333333730697632, 0.7611939907073975, 0.5249999761581421]\n",
            "Mean test per-class accuracy at best validation: [0.8107476830482483, 0.8541666269302368, 0.3125, 0.7040815949440002, 0.4583333432674408, 0.8214285969734192, 0.7534483075141907, 0.6818181872367859]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}