{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capsule_Network_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZvdAUbRrX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import itertools as it"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZatW3jZwR-38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b93dae2c-131c-4306-aaa6-66fde718e2ac"
      },
      "source": [
        "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available? True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTYBpe-9SA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79776ce3-0a22-4a77-a4ff-fe486b497fa4"
      },
      "source": [
        "drive.mount(\"/content/drive\",True)\n",
        "root_dir = \"/content/drive/My Drive/SB3/\""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rldL-WKSBuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTAga3c5SGos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "660df16b-cfc8-46f1-b156-c3461c1fc7b6"
      },
      "source": [
        "train_dataset = ImageFolder(os.path.join(root_dir, \"train\"), transform=train_transform)\n",
        "test_dataset = ImageFolder(os.path.join(root_dir, \"test\"), transform=test_transform)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA59yhfx3rj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "32ae32f7-cf19-4135-f927-f0c42b0ef00a"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIXTmrPp0C7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eba537ba-3112-49b2-9115-74f407bb3103"
      },
      "source": [
        "x = train_dataset[0][0]\n",
        "print(x.max())\n",
        "print(x.min())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9961)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqDBfdoG0DXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98b63181-b762-4250-92d7-c7662cf934e6"
      },
      "source": [
        "class_counts = [train_dataset.targets.count(i) for i in range(num_classes)]\n",
        "print(class_counts)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[500, 28, 38, 116, 17, 19, 339, 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wJ1KbeYthE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = len(train_dataset)\n",
        "idx = list(range(num_train))\n",
        "val_frac = 0.2\n",
        "idx_iter = iter(idx)\n",
        "class_idx = [list(it.islice(idx_iter, x)) for x in class_counts]\n",
        "class_idx = [random.sample(x,len(x)) for x in class_idx]\n",
        "train_idx = [x[:-int(len(x)*val_frac)] for x in class_idx]\n",
        "train_idx = list(it.chain.from_iterable(train_idx))\n",
        "val_idx = [x[-int(len(x)*val_frac):] for x in class_idx]\n",
        "val_idx = list(it.chain.from_iterable(val_idx))\n",
        "val_dataset = Subset(train_dataset,val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt7N8rXH-PYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b59fe1a0-a5b2-4fca-a0c6-12d277c455c4"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891\n",
            "218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cenTm_mFN_0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "74a0cd37-5f63-4ff8-9d26-320981807ab1"
      },
      "source": [
        "train_counts = [x-int(x*val_frac) for x in class_counts]\n",
        "max_count = max(train_counts)\n",
        "weights = max_count / torch.Tensor(train_counts)\n",
        "weights = weights.to(dev)\n",
        "print(weights)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.0000, 17.3913, 12.9032,  4.3011, 28.5714, 25.0000,  1.4706,  9.5238],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ntOGeoh_cS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b25a8efb-dab8-4f3f-8dfc-b1817190908e"
      },
      "source": [
        "print(max_count)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9SIeKL_SMm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset,   batch_size=4, num_workers=4, shuffle=False)\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\" : val_loader,\n",
        "    \"test\": test_loader\n",
        "}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgR4FQ2tU2jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, dim=-1):\n",
        "\t'''\n",
        "\t\"Squashing\" non-linearity that shrunks short vectors to almost zero length and long vectors to a length slightly below 1\n",
        "\tEq. (1): v_j = ||s_j||^2 / (1 + ||s_j||^2) * s_j / ||s_j||\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ts: \tVector before activation\n",
        "\t\tdim:\tDimension along which to calculate the norm\n",
        "\t\n",
        "\tReturns:\n",
        "\t\tSquashed vector\n",
        "\t'''\n",
        "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
        "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q1JXyMoSQKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, vector_length, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Initialize the layer.\n",
        "    Args:\n",
        "      in_channels: \tNumber of input channels.\n",
        "      out_channels: \tNumber of output channels.\n",
        "      vector_length:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vector_length = vector_length\n",
        "    self.num_caps_channels = int(out_channels / vector_length)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), self.num_caps_channels, x.size(2), x.size(3), self.vector_length)\n",
        "    x = x.view(x.size(0), -1, self.vector_length)\n",
        "    return squash(x)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6uiDtWVSmUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoutingCapsules(nn.Module):\n",
        "  def __init__(self, in_vector_length, num_in_caps, num_out_caps, out_vector_length, num_routing):\n",
        "    '''\n",
        "\t\tInitialize the layer.\n",
        "\t\tArgs:\n",
        "\t\t\tin_vector_length: \t\tDimensionality (i.e. length) of each capsule vector.\n",
        "\t\t\tnum_in_caps: \t\tNumber of input capsules if digits layer.\n",
        "\t\t\tnum_out_caps: \t\tNumber of capsules in the capsule layer\n",
        "\t\t\tout_vector_length: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\t\t\tnum_routing:\tNumber of iterations during routing algorithm\t\t\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.in_vector_length = in_vector_length\n",
        "    self.num_in_caps = num_in_caps\n",
        "    self.num_out_caps = num_out_caps\n",
        "    self.out_vector_length = out_vector_length\n",
        "    self.num_routing = num_routing\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(1, num_out_caps, num_in_caps, out_vector_length, in_vector_length ) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    # (batch_size, num_in_caps, in_vector_length) -> (batch_size, 1, num_in_caps, in_vector_length, 1)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    #\n",
        "    # W @ x =\n",
        "    # (1, num_output_caps, num_in_caps, out_vector_length, in_vector_length) @ (batch_size, 1, num_in_caps, in_vector_length, 1) =\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    # detach u_hat during routing iterations to prevent gradients from flowing\n",
        "    temp_u_hat = u_hat.detach()\n",
        "\n",
        "    '''\n",
        "    Procedure 1: Routing algorithm\n",
        "    '''\n",
        "    b = torch.zeros(batch_size, self.num_out_caps, self.num_in_caps, 1).to(dev)\n",
        "\n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) -> Softmax along num_out_caps\n",
        "      c = F.softmax(b, dim=1)\n",
        "\n",
        "      # element-wise multiplication\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) * (batch_size, num_in_caps, num_out_caps, out_vector_length) ->\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) sum across num_in_caps ->\n",
        "      # (batch_size, num_out_caps, out_vector_length)\n",
        "      s = (c * temp_u_hat).sum(dim=2)\n",
        "      # apply \"squashing\" non-linearity along dim_caps\n",
        "      v = squash(s)\n",
        "      # dot product agreement between the current output vj and the prediction uj|i\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) @ (batch_size, num_out_caps, out_vector_length, 1)\n",
        "      # -> (batch_size, num_out_caps, num_in_caps, 1)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "\n",
        "    # last iteration is done on the original u_hat, without the routing weights update\n",
        "    c = F.softmax(b, dim=1)\n",
        "    s = (c * u_hat).sum(dim=2)\n",
        "    # apply \"squashing\" non-linearity along dim_caps\n",
        "    v = squash(s)\n",
        "\n",
        "    return v"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZmQKyYPSQST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, num_classes, vector_length):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(num_classes*vector_length, 64),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.reconstruction_layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(1, 128, kernel_size=5, padding=2, stride=11),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2, stride=4, output_padding=(1,1)),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.Conv2d(64, 3, kernel_size=5, padding=7, stride=1),\n",
        "          nn.Sigmoid()\n",
        "    )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.fc_layer(x)\n",
        "      x = x.view(x.size(0), 1, 8, 8)\n",
        "      x = self.reconstruction_layers(x)\n",
        "      return x"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWxbVp1SRBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=5, padding=2, stride=4),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.MaxPool2d(kernel_size=5, padding=2, stride=2)\n",
        "      )\n",
        "      self.primary_caps = PrimaryCapsules(in_channels=64, out_channels=128, vector_length=8, kernel_size=5, padding=2, stride=1)\n",
        "      self.digit_caps = nn.Sequential(\n",
        "        RoutingCapsules(in_vector_length=8, num_in_caps=25600, num_out_caps=20, out_vector_length=16, num_routing=3),\n",
        "        RoutingCapsules(in_vector_length=16, num_in_caps=20, num_out_caps=num_classes, out_vector_length=16, num_routing=3)\n",
        "      )\n",
        "      self.reconstruction_layer = Reconstruction(num_classes=num_classes, vector_length=16)\n",
        "    \n",
        "    '''\n",
        "    def capsule_average_pooling(self, x):\n",
        "      height = x.size(3)\n",
        "      width = x.size(4)\n",
        "      x = x.sum(dim=4).sum(dim=3)\n",
        "      x = x / (height * width)\n",
        "      return x\n",
        "    '''\n",
        "    \n",
        "    def score(self, x):\n",
        "      return torch.sqrt((x ** 2).sum(dim=2))\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.conv_layer(x)\n",
        "      x = self.primary_caps(x)\n",
        "      x = self.digit_caps(x)\n",
        "      scores = self.score(x)\n",
        "      x = x.view(x.size(0), x.size(1) * x.size(2))\n",
        "      reconstructions = self.reconstruction_layer(x)\n",
        "      return scores, reconstructions"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9EcOQTmYI3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CapsNet(num_classes=num_classes)\n",
        "model = model.to(dev)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghnqtQeaxam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8d3cd91f-add8-4aea-ae4c-2c75db50c296"
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "scores, reconstructions = model(batch)\n",
        "print(scores.size())\n",
        "print(reconstructions.size())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 3, 320, 320])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_iemIRmlwd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, dev, lr=0.001):\n",
        "    try:\n",
        "        # Create model\n",
        "        model = CapsNet(num_classes=num_classes)\n",
        "        model = model.to(dev)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        # Initialize history\n",
        "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        # Process each epoch\n",
        "        for epoch in range(epochs):\n",
        "            # Initialize epoch variables\n",
        "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            \n",
        "            # Process each split\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                  model.train()\n",
        "                else:\n",
        "                  model.eval()\n",
        "                # Process each batch\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    # Move to CUDA\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    # Reset gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # Compute output\n",
        "                    pred, reconstructions = model(input)\n",
        "                    score_loss = F.cross_entropy(pred, labels, weight=weights)\n",
        "                    reconstruction_loss = F.mse_loss(input, reconstructions)\n",
        "                    loss = score_loss + reconstruction_loss\n",
        "                    # Update loss\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    # Check parameter update\n",
        "                    if split == \"train\":\n",
        "                        # Compute gradients\n",
        "                        loss.backward()\n",
        "                        # Optimize\n",
        "                        optimizer.step()\n",
        "                    # Compute accuracy\n",
        "                    _,pred_labels = pred.max(1)\n",
        "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
        "                    # Update accuracy\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "            # Compute epoch loss/accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            # Update history\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "            # Print info\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
        "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"VL={epoch_loss['val']:.4f},\",\n",
        "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
        "                  f\"TeA={epoch_accuracy['test']:.4f},\"\n",
        "                )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        # Plot loss\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # Plot accuracy\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcTyH7emXjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "c56b6c5e-da68-41b5-f794-6e4a35a1f890"
      },
      "source": [
        "train(100, dev, lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: TrL=2.0380, TrA=0.3288, VL=1.8934, VA=0.3455, TeL=1.9014, TeA=0.3432,\n",
            "Epoch 2: TrL=1.9438, TrA=0.3677, VL=1.8198, VA=0.4409, TeL=1.8103, TeA=0.4703,\n",
            "Epoch 3: TrL=1.9001, TrA=0.4178, VL=1.7834, VA=0.4409, TeL=1.7863, TeA=0.4703,\n",
            "Epoch 4: TrL=1.8711, TrA=0.4641, VL=1.8145, VA=0.3318, TeL=1.8085, TeA=0.3708,\n",
            "Epoch 5: TrL=1.8585, TrA=0.4600, VL=1.7752, VA=0.4636, TeL=1.7682, TeA=0.5085,\n",
            "Epoch 6: TrL=1.8204, TrA=0.4458, VL=1.7075, VA=0.4591, TeL=1.7134, TeA=0.5000,\n",
            "Epoch 7: TrL=1.8236, TrA=0.4735, VL=1.7680, VA=0.4955, TeL=1.7681, TeA=0.5106,\n",
            "Epoch 8: TrL=1.7842, TrA=0.4540, VL=1.7028, VA=0.4500, TeL=1.7053, TeA=0.5042,\n",
            "Epoch 9: TrL=1.7483, TrA=0.5000, VL=1.6766, VA=0.4727, TeL=1.6772, TeA=0.5106,\n",
            "Epoch 10: TrL=1.7273, TrA=0.4854, VL=1.6922, VA=0.4682, TeL=1.6818, TeA=0.5169,\n",
            "Epoch 11: TrL=1.6946, TrA=0.4888, VL=1.6764, VA=0.4909, TeL=1.6707, TeA=0.5169,\n",
            "Epoch 12: TrL=1.7138, TrA=0.4410, VL=1.7028, VA=0.3591, TeL=1.6956, TeA=0.4131,\n",
            "Epoch 13: TrL=1.7463, TrA=0.3670, VL=1.6965, VA=0.3818, TeL=1.6961, TeA=0.4322,\n",
            "Epoch 14: TrL=1.7162, TrA=0.4559, VL=1.6743, VA=0.4864, TeL=1.6793, TeA=0.5021,\n",
            "Epoch 15: TrL=1.6712, TrA=0.4851, VL=1.6612, VA=0.4955, TeL=1.6605, TeA=0.5127,\n",
            "Epoch 16: TrL=1.6587, TrA=0.4918, VL=1.6577, VA=0.4955, TeL=1.6555, TeA=0.5148,\n",
            "Epoch 17: TrL=1.6271, TrA=0.4903, VL=1.6650, VA=0.4955, TeL=1.6507, TeA=0.5148,\n",
            "Epoch 18: TrL=1.6057, TrA=0.4899, VL=1.6635, VA=0.4955, TeL=1.6460, TeA=0.5127,\n",
            "Epoch 19: TrL=1.6231, TrA=0.4907, VL=1.6362, VA=0.4955, TeL=1.6457, TeA=0.5148,\n",
            "Epoch 20: TrL=1.5874, TrA=0.4914, VL=1.6385, VA=0.4955, TeL=1.6415, TeA=0.5169,\n",
            "Epoch 21: TrL=1.5717, TrA=0.4892, VL=1.6352, VA=0.4955, TeL=1.6170, TeA=0.5148,\n",
            "Epoch 22: TrL=1.5827, TrA=0.4929, VL=1.6431, VA=0.4955, TeL=1.6375, TeA=0.5169,\n",
            "Epoch 23: TrL=1.5547, TrA=0.4813, VL=1.6205, VA=0.4955, TeL=1.6154, TeA=0.5127,\n",
            "Epoch 24: TrL=1.5415, TrA=0.4929, VL=1.5967, VA=0.5000, TeL=1.5949, TeA=0.5127,\n",
            "Epoch 25: TrL=1.5286, TrA=0.4892, VL=1.5963, VA=0.4909, TeL=1.5859, TeA=0.5085,\n",
            "Epoch 26: TrL=1.5236, TrA=0.4944, VL=1.5912, VA=0.5045, TeL=1.6009, TeA=0.5064,\n",
            "Epoch 27: TrL=1.5368, TrA=0.5407, VL=1.6017, VA=0.5318, TeL=1.6054, TeA=0.5530,\n",
            "Epoch 28: TrL=1.5349, TrA=0.6091, VL=1.5911, VA=0.5318, TeL=1.6046, TeA=0.5339,\n",
            "Epoch 29: TrL=1.5359, TrA=0.6420, VL=1.6089, VA=0.5136, TeL=1.6156, TeA=0.5487,\n",
            "Epoch 30: TrL=1.5097, TrA=0.6868, VL=1.6073, VA=0.5318, TeL=1.6036, TeA=0.5869,\n",
            "Epoch 31: TrL=1.4908, TrA=0.7261, VL=1.7205, VA=0.5000, TeL=1.7082, TeA=0.5847,\n",
            "Epoch 32: TrL=1.4909, TrA=0.7123, VL=1.6085, VA=0.6091, TeL=1.6320, TeA=0.6059,\n",
            "Epoch 33: TrL=1.4754, TrA=0.7668, VL=1.6103, VA=0.5955, TeL=1.6034, TeA=0.5975,\n",
            "Epoch 34: TrL=1.4575, TrA=0.7586, VL=1.6004, VA=0.5455, TeL=1.5953, TeA=0.5911,\n",
            "Epoch 35: TrL=1.4591, TrA=0.7664, VL=1.6046, VA=0.5864, TeL=1.5962, TeA=0.6419,\n",
            "Epoch 36: TrL=1.4521, TrA=0.7698, VL=1.6444, VA=0.5909, TeL=1.6341, TeA=0.6208,\n",
            "Epoch 37: TrL=1.4253, TrA=0.7971, VL=1.6103, VA=0.5727, TeL=1.5946, TeA=0.6081,\n",
            "Epoch 38: TrL=1.4131, TrA=0.8363, VL=1.5715, VA=0.5955, TeL=1.5713, TeA=0.6419,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}