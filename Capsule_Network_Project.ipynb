{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capsule_Network_Project_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZvdAUbRrX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import itertools as it"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZatW3jZwR-38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4cbb0305-01f7-46ed-e215-b887cb46f0ec"
      },
      "source": [
        "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available? True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTYBpe-9SA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74f1d97f-dcb1-4b96-d309-51e1e1fafb27"
      },
      "source": [
        "drive.mount(\"/content/drive\",True)\n",
        "root_dir = \"/content/drive/My Drive/SB3/\""
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rldL-WKSBuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTAga3c5SGos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac980664-9557-424d-97ae-fe9c67e0a7fb"
      },
      "source": [
        "train_dataset = ImageFolder(os.path.join(root_dir, \"train\"), transform=train_transform)\n",
        "test_dataset = ImageFolder(os.path.join(root_dir, \"test\"), transform=test_transform)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA59yhfx3rj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "592194d4-a169-4dc3-ea70-fb0cec157fbe"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIXTmrPp0C7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8a0aa965-c661-417d-ff43-430beb5f647d"
      },
      "source": [
        "x = train_dataset[0][0]\n",
        "print(x.max())\n",
        "print(x.min())"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9961)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wJ1KbeYthE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_counts = [train_dataset.targets.count(i) for i in range(num_classes)]\n",
        "num_train = len(train_dataset)\n",
        "idx = list(range(num_train))\n",
        "val_frac = 0.2\n",
        "idx_iter = iter(idx)\n",
        "class_idx = [list(it.islice(idx_iter, x)) for x in class_counts]\n",
        "train_idx = [x[:-int(len(x)*val_frac)] for x in class_idx]\n",
        "train_idx = list(it.chain.from_iterable(train_idx))\n",
        "val_idx = [x[-int(len(x)*val_frac):] for x in class_idx]\n",
        "val_idx = list(it.chain.from_iterable(val_idx))\n",
        "val_dataset = Subset(train_dataset,val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt7N8rXH-PYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f92c79c6-2056-476c-c215-04465750428d"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "891\n",
            "218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P10rGKk8SIW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "788389af-cc79-4c3f-cd81-2757b4e20e11"
      },
      "source": [
        "'''\n",
        "num_train = len(train_dataset)\n",
        "train_idx = list(range(num_train))\n",
        "random.shuffle(train_idx)\n",
        "val_frac = 0.3\n",
        "num_val = int(num_train*val_frac)\n",
        "num_train = num_train - num_val\n",
        "val_idx = train_idx[num_train:]\n",
        "train_idx = train_idx[:num_train]\n",
        "val_dataset = Subset(train_dataset,val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)\n",
        "'''"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nnum_train = len(train_dataset)\\ntrain_idx = list(range(num_train))\\nrandom.shuffle(train_idx)\\nval_frac = 0.3\\nnum_val = int(num_train*val_frac)\\nnum_train = num_train - num_val\\nval_idx = train_idx[num_train:]\\ntrain_idx = train_idx[:num_train]\\nval_dataset = Subset(train_dataset,val_idx)\\ntrain_dataset = Subset(train_dataset, train_idx)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cenTm_mFN_0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_counts = [x-int(x*val_frac) for x in class_counts]\n",
        "weights = len(train_dataset) / torch.Tensor(train_counts)\n",
        "weights = weights.to(dev)\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9SIeKL_SMm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset,   batch_size=4, num_workers=4, shuffle=False)\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\" : val_loader,\n",
        "    \"test\": test_loader\n",
        "}"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgR4FQ2tU2jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, dim=-1):\n",
        "\t'''\n",
        "\t\"Squashing\" non-linearity that shrunks short vectors to almost zero length and long vectors to a length slightly below 1\n",
        "\tEq. (1): v_j = ||s_j||^2 / (1 + ||s_j||^2) * s_j / ||s_j||\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ts: \tVector before activation\n",
        "\t\tdim:\tDimension along which to calculate the norm\n",
        "\t\n",
        "\tReturns:\n",
        "\t\tSquashed vector\n",
        "\t'''\n",
        "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
        "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q1JXyMoSQKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, vector_length, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Initialize the layer.\n",
        "    Args:\n",
        "      in_channels: \tNumber of input channels.\n",
        "      out_channels: \tNumber of output channels.\n",
        "      vector_length:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vector_length = vector_length\n",
        "    self.num_caps_channels = int(out_channels / vector_length)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), self.num_caps_channels, x.size(2), x.size(3), self.vector_length)\n",
        "    x = x.view(x.size(0), -1, self.vector_length)\n",
        "    return squash(x)\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6uiDtWVSmUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoutingCapsules(nn.Module):\n",
        "  def __init__(self, in_vector_length, num_in_caps, num_out_caps, out_vector_length, num_routing):\n",
        "    '''\n",
        "\t\tInitialize the layer.\n",
        "\t\tArgs:\n",
        "\t\t\tin_vector_length: \t\tDimensionality (i.e. length) of each capsule vector.\n",
        "\t\t\tnum_in_caps: \t\tNumber of input capsules if digits layer.\n",
        "\t\t\tnum_out_caps: \t\tNumber of capsules in the capsule layer\n",
        "\t\t\tout_vector_length: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\t\t\tnum_routing:\tNumber of iterations during routing algorithm\t\t\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.in_vector_length = in_vector_length\n",
        "    self.num_in_caps = num_in_caps\n",
        "    self.num_out_caps = num_out_caps\n",
        "    self.out_vector_length = out_vector_length\n",
        "    self.num_routing = num_routing\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(1, num_out_caps, num_in_caps, out_vector_length, in_vector_length ) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    # (batch_size, num_in_caps, in_vector_length) -> (batch_size, 1, num_in_caps, in_vector_length, 1)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    #\n",
        "    # W @ x =\n",
        "    # (1, num_output_caps, num_in_caps, out_vector_length, in_vector_length) @ (batch_size, 1, num_in_caps, in_vector_length, 1) =\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    # detach u_hat during routing iterations to prevent gradients from flowing\n",
        "    temp_u_hat = u_hat.detach()\n",
        "\n",
        "    '''\n",
        "    Procedure 1: Routing algorithm\n",
        "    '''\n",
        "    b = torch.zeros(batch_size, self.num_out_caps, self.num_in_caps, 1).to(dev)\n",
        "\n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) -> Softmax along num_out_caps\n",
        "      c = F.softmax(b, dim=1)\n",
        "\n",
        "      # element-wise multiplication\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) * (batch_size, num_in_caps, num_out_caps, out_vector_length) ->\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) sum across num_in_caps ->\n",
        "      # (batch_size, num_out_caps, out_vector_length)\n",
        "      s = (c * temp_u_hat).sum(dim=2)\n",
        "      # apply \"squashing\" non-linearity along dim_caps\n",
        "      v = squash(s)\n",
        "      # dot product agreement between the current output vj and the prediction uj|i\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) @ (batch_size, num_out_caps, out_vector_length, 1)\n",
        "      # -> (batch_size, num_out_caps, num_in_caps, 1)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "\n",
        "    # last iteration is done on the original u_hat, without the routing weights update\n",
        "    c = F.softmax(b, dim=1)\n",
        "    s = (c * u_hat).sum(dim=2)\n",
        "    # apply \"squashing\" non-linearity along dim_caps\n",
        "    v = squash(s)\n",
        "\n",
        "    return v"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZmQKyYPSQST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, num_classes, vector_length):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(num_classes*vector_length, 64),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.reconstruction_layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(1, 128, kernel_size=5, padding=2, stride=11),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2, stride=4, output_padding=(1,1)),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(64, 3, kernel_size=5, padding=7, stride=1),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.fc_layer(x)\n",
        "      x = x.view(x.size(0), 1, 8, 8)\n",
        "      x = self.reconstruction_layers(x)\n",
        "      return x"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWxbVp1SRBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=5, padding=2, stride=8),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.primary_caps = PrimaryCapsules(in_channels=64, out_channels=128, vector_length=8, kernel_size=5, padding=2, stride=1)\n",
        "      self.digit_caps = nn.Sequential(\n",
        "        RoutingCapsules(in_vector_length=8, num_in_caps=25600, num_out_caps=20, out_vector_length=16, num_routing=3),\n",
        "        RoutingCapsules(in_vector_length=16, num_in_caps=20, num_out_caps=num_classes, out_vector_length=16, num_routing=3)\n",
        "      )\n",
        "      self.reconstruction_layer = Reconstruction(num_classes=num_classes, vector_length=16)\n",
        "    \n",
        "    '''\n",
        "    def capsule_average_pooling(self, x):\n",
        "      height = x.size(3)\n",
        "      width = x.size(4)\n",
        "      x = x.sum(dim=4).sum(dim=3)\n",
        "      x = x / (height * width)\n",
        "      return x\n",
        "    '''\n",
        "    \n",
        "    def score(self, x):\n",
        "      return torch.sqrt((x ** 2).sum(dim=2))\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.conv_layer(x)\n",
        "      x = self.primary_caps(x)\n",
        "      x = self.digit_caps(x)\n",
        "      scores = self.score(x)\n",
        "      x = x.view(x.size(0), x.size(1) * x.size(2))\n",
        "      reconstructions = self.reconstruction_layer(x)\n",
        "      return scores, reconstructions"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9EcOQTmYI3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CapsNet(num_classes=num_classes)\n",
        "model = model.to(dev)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghnqtQeaxam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5ba457c7-d04c-4bf5-a065-a032b5edf54f"
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "#print(batch.size())\n",
        "scores, reconstructions = model(batch)\n",
        "print(scores.size())\n",
        "print(reconstructions.size())"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 3, 320, 320])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXC9UxPY0gnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "#print(batch.size())\n",
        "out = model(batch)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_iemIRmlwd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, dev, lr=0.001):\n",
        "    try:\n",
        "        # Create model\n",
        "        model = CapsNet(num_classes=num_classes)\n",
        "        model = model.to(dev)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        # Initialize history\n",
        "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        # Process each epoch\n",
        "        for epoch in range(epochs):\n",
        "            # Initialize epoch variables\n",
        "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            \n",
        "            # Process each split\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                  model.train()\n",
        "                else:\n",
        "                  model.eval()\n",
        "                # Process each batch\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    # Move to CUDA\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    # Reset gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # Compute output\n",
        "                    pred, reconstructions = model(input)\n",
        "                    score_loss = F.cross_entropy(pred, labels, weight=weights)\n",
        "                    reconstruction_loss = F.mse_loss(input, reconstructions)\n",
        "                    loss = score_loss + reconstruction_loss\n",
        "                    # Update loss\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    # Check parameter update\n",
        "                    if split == \"train\":\n",
        "                        # Compute gradients\n",
        "                        loss.backward()\n",
        "                        # Optimize\n",
        "                        optimizer.step()\n",
        "                    # Compute accuracy\n",
        "                    _,pred_labels = pred.max(1)\n",
        "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
        "                    # Update accuracy\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "            # Compute epoch loss/accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            # Update history\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "            # Print info\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
        "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"VL={epoch_loss['val']:.4f},\",\n",
        "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
        "                  f\"TeA={epoch_accuracy['test']:.4f},\"\n",
        "                )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        # Plot loss\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # Plot accuracy\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcTyH7emXjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7b709222-8805-491d-b1b8-8517d35f9f91"
      },
      "source": [
        "train(100, dev, lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: TrL=2.0520, TrA=0.3255, VL=1.9159, VA=0.4545, TeL=1.9006, TeA=0.4534,\n",
            "Epoch 2: TrL=2.0117, TrA=0.3980, VL=1.8926, VA=0.3364, TeL=1.8784, TeA=0.3750,\n",
            "Epoch 3: TrL=2.0209, TrA=0.3729, VL=1.9328, VA=0.3091, TeL=1.9130, TeA=0.3093,\n",
            "Epoch 4: TrL=2.0327, TrA=0.3274, VL=1.9414, VA=0.3045, TeL=1.9150, TeA=0.3072,\n",
            "Epoch 5: TrL=2.0156, TrA=0.3049, VL=1.9441, VA=0.3045, TeL=1.9254, TeA=0.3072,\n",
            "Epoch 6: TrL=1.9737, TrA=0.3154, VL=1.9320, VA=0.5045, TeL=1.9114, TeA=0.4386,\n",
            "Epoch 7: TrL=1.9797, TrA=0.3558, VL=1.9389, VA=0.4545, TeL=1.9213, TeA=0.4534,\n",
            "Epoch 8: TrL=1.9773, TrA=0.4301, VL=1.8683, VA=0.5591, TeL=1.8475, TeA=0.4894,\n",
            "Epoch 9: TrL=1.9259, TrA=0.4387, VL=1.8975, VA=0.5636, TeL=1.8668, TeA=0.5000,\n",
            "Epoch 10: TrL=1.9277, TrA=0.3419, VL=1.8901, VA=0.3273, TeL=1.8686, TeA=0.3114,\n",
            "Epoch 11: TrL=1.8925, TrA=0.3868, VL=1.7777, VA=0.4364, TeL=1.7269, TeA=0.4322,\n",
            "Epoch 12: TrL=1.8308, TrA=0.4436, VL=1.7788, VA=0.3091, TeL=1.7495, TeA=0.3072,\n",
            "Epoch 13: TrL=1.7797, TrA=0.3307, VL=1.7492, VA=0.3364, TeL=1.6879, TeA=0.3644,\n",
            "Epoch 14: TrL=1.7324, TrA=0.3348, VL=1.7710, VA=0.5636, TeL=1.6859, TeA=0.4809,\n",
            "Epoch 15: TrL=1.6726, TrA=0.4051, VL=1.7801, VA=0.3182, TeL=1.6799, TeA=0.3411,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}