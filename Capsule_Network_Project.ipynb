{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capsule_Network_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZvdAUbRrX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import itertools as it"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZatW3jZwR-38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e0e012b1-dd07-4e6a-9210-6b32af050688"
      },
      "source": [
        "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(dev)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available? True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTYBpe-9SA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ad69fe3-339d-4459-fe93-5a1be2825569"
      },
      "source": [
        "drive.mount(\"/content/drive\",True)\n",
        "root_dir = \"/content/drive/My Drive/SB3/\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rldL-WKSBuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(320),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTAga3c5SGos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04b25662-b0a8-49d9-9a9d-2885abb3c2ae"
      },
      "source": [
        "train_dataset = ImageFolder(os.path.join(root_dir, \"train\"), transform=train_transform)\n",
        "test_dataset = ImageFolder(os.path.join(root_dir, \"test\"), transform=test_transform)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA59yhfx3rj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "605439d3-ba0b-432a-b88a-e1aab362a9bc"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIXTmrPp0C7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b1f4a526-73b6-4fce-963f-27dd236a2d5e"
      },
      "source": [
        "x = train_dataset[0][0]\n",
        "print(x.max())\n",
        "print(x.min())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9961)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqDBfdoG0DXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e08bec3a-cf5c-46f9-8521-db3c85f3f1f8"
      },
      "source": [
        "class_counts = [train_dataset.targets.count(i) for i in range(num_classes)]\n",
        "print(class_counts)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[500, 28, 38, 116, 17, 19, 339, 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wJ1KbeYthE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = len(train_dataset)\n",
        "idx = list(range(num_train))\n",
        "val_frac = 0.3\n",
        "idx_iter = iter(idx)\n",
        "class_idx = [list(it.islice(idx_iter, x)) for x in class_counts]\n",
        "class_idx = [random.sample(x,len(x)) for x in class_idx]\n",
        "train_idx = [x[:-int(len(x)*val_frac)] for x in class_idx]\n",
        "train_idx = list(it.chain.from_iterable(train_idx))\n",
        "val_idx = [x[-int(len(x)*val_frac):] for x in class_idx]\n",
        "val_idx = list(it.chain.from_iterable(val_idx))\n",
        "val_dataset = Subset(train_dataset,val_idx)\n",
        "train_dataset = Subset(train_dataset, train_idx)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt7N8rXH-PYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "de43857a-e8cb-4572-a043-d59d6e4cb3df"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "780\n",
            "329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cenTm_mFN_0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f2c97419-f2db-4ea5-f1cc-ed3ad5716ec4"
      },
      "source": [
        "train_counts = [x-int(x*val_frac) for x in class_counts]\n",
        "max_count = max(train_counts)\n",
        "weights = max_count / torch.Tensor(train_counts)\n",
        "weights = weights.to(dev)\n",
        "print(weights)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.0000, 17.5000, 12.9630,  4.2683, 29.1667, 25.0000,  1.4706,  9.4595],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ntOGeoh_cS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b84c851-b570-422a-f824-9f42109aef3d"
      },
      "source": [
        "print(max_count)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9SIeKL_SMm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, num_workers=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset,   batch_size=4, num_workers=4, shuffle=False)\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\" : val_loader,\n",
        "    \"test\": test_loader\n",
        "}"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgR4FQ2tU2jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(s, dim=-1):\n",
        "\t'''\n",
        "\t\"Squashing\" non-linearity that shrunks short vectors to almost zero length and long vectors to a length slightly below 1\n",
        "\tEq. (1): v_j = ||s_j||^2 / (1 + ||s_j||^2) * s_j / ||s_j||\n",
        "\t\n",
        "\tArgs:\n",
        "\t\ts: \tVector before activation\n",
        "\t\tdim:\tDimension along which to calculate the norm\n",
        "\t\n",
        "\tReturns:\n",
        "\t\tSquashed vector\n",
        "\t'''\n",
        "\tsquared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
        "\treturn squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q1JXyMoSQKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, vector_length, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Initialize the layer.\n",
        "    Args:\n",
        "      in_channels: \tNumber of input channels.\n",
        "      out_channels: \tNumber of output channels.\n",
        "      vector_length:\t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vector_length = vector_length\n",
        "    self.num_caps_channels = int(out_channels / vector_length)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), self.num_caps_channels, x.size(2), x.size(3), self.vector_length)\n",
        "    x = x.view(x.size(0), -1, self.vector_length)\n",
        "    return squash(x)\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6uiDtWVSmUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoutingCapsules(nn.Module):\n",
        "  def __init__(self, in_vector_length, num_in_caps, num_out_caps, out_vector_length, num_routing):\n",
        "    '''\n",
        "\t\tInitialize the layer.\n",
        "\t\tArgs:\n",
        "\t\t\tin_vector_length: \t\tDimensionality (i.e. length) of each capsule vector.\n",
        "\t\t\tnum_in_caps: \t\tNumber of input capsules if digits layer.\n",
        "\t\t\tnum_out_caps: \t\tNumber of capsules in the capsule layer\n",
        "\t\t\tout_vector_length: \t\tDimensionality, i.e. length, of the output capsule vector.\n",
        "\t\t\tnum_routing:\tNumber of iterations during routing algorithm\t\t\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.in_vector_length = in_vector_length\n",
        "    self.num_in_caps = num_in_caps\n",
        "    self.num_out_caps = num_out_caps\n",
        "    self.out_vector_length = out_vector_length\n",
        "    self.num_routing = num_routing\n",
        "\n",
        "    self.W = nn.Parameter(torch.randn(1, num_out_caps, num_in_caps, out_vector_length, in_vector_length ) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    # (batch_size, num_in_caps, in_vector_length) -> (batch_size, 1, num_in_caps, in_vector_length, 1)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    #\n",
        "    # W @ x =\n",
        "    # (1, num_output_caps, num_in_caps, out_vector_length, in_vector_length) @ (batch_size, 1, num_in_caps, in_vector_length, 1) =\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    # (batch_size, num_out_caps, num_in_caps, out_vector_length)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    # detach u_hat during routing iterations to prevent gradients from flowing\n",
        "    temp_u_hat = u_hat.detach()\n",
        "\n",
        "    '''\n",
        "    Procedure 1: Routing algorithm\n",
        "    '''\n",
        "    b = torch.zeros(batch_size, self.num_out_caps, self.num_in_caps, 1).to(dev)\n",
        "\n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) -> Softmax along num_out_caps\n",
        "      c = F.softmax(b, dim=1)\n",
        "\n",
        "      # element-wise multiplication\n",
        "      # (batch_size, num_out_caps, num_in_caps, 1) * (batch_size, num_in_caps, num_out_caps, out_vector_length) ->\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) sum across num_in_caps ->\n",
        "      # (batch_size, num_out_caps, out_vector_length)\n",
        "      s = (c * temp_u_hat).sum(dim=2)\n",
        "      # apply \"squashing\" non-linearity along dim_caps\n",
        "      v = squash(s)\n",
        "      # dot product agreement between the current output vj and the prediction uj|i\n",
        "      # (batch_size, num_out_caps, num_in_caps, out_vector_length) @ (batch_size, num_out_caps, out_vector_length, 1)\n",
        "      # -> (batch_size, num_out_caps, num_in_caps, 1)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "\n",
        "    # last iteration is done on the original u_hat, without the routing weights update\n",
        "    c = F.softmax(b, dim=1)\n",
        "    s = (c * u_hat).sum(dim=2)\n",
        "    # apply \"squashing\" non-linearity along dim_caps\n",
        "    v = squash(s)\n",
        "\n",
        "    return v"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZmQKyYPSQST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, num_classes, vector_length):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(num_classes*vector_length, 64),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.reconstruction_layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(1, 128, kernel_size=5, padding=2, stride=11),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=5, padding=2, stride=4, output_padding=(1,1)),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(64, 3, kernel_size=5, padding=7, stride=1),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.fc_layer(x)\n",
        "      x = x.view(x.size(0), 1, 8, 8)\n",
        "      x = self.reconstruction_layers(x)\n",
        "      return x"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qWxbVp1SRBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super().__init__()\n",
        "      self.conv_layer = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=5, padding=2, stride=8),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.primary_caps = PrimaryCapsules(in_channels=64, out_channels=128, vector_length=8, kernel_size=5, padding=2, stride=1)\n",
        "      self.digit_caps = nn.Sequential(\n",
        "        RoutingCapsules(in_vector_length=8, num_in_caps=25600, num_out_caps=20, out_vector_length=16, num_routing=3),\n",
        "        RoutingCapsules(in_vector_length=16, num_in_caps=20, num_out_caps=num_classes, out_vector_length=16, num_routing=3)\n",
        "      )\n",
        "      self.reconstruction_layer = Reconstruction(num_classes=num_classes, vector_length=16)\n",
        "    \n",
        "    '''\n",
        "    def capsule_average_pooling(self, x):\n",
        "      height = x.size(3)\n",
        "      width = x.size(4)\n",
        "      x = x.sum(dim=4).sum(dim=3)\n",
        "      x = x / (height * width)\n",
        "      return x\n",
        "    '''\n",
        "    \n",
        "    def score(self, x):\n",
        "      return torch.sqrt((x ** 2).sum(dim=2))\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.conv_layer(x)\n",
        "      x = self.primary_caps(x)\n",
        "      x = self.digit_caps(x)\n",
        "      scores = self.score(x)\n",
        "      x = x.view(x.size(0), x.size(1) * x.size(2))\n",
        "      reconstructions = self.reconstruction_layer(x)\n",
        "      return scores, reconstructions"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9EcOQTmYI3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CapsNet(num_classes=num_classes)\n",
        "model = model.to(dev)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghnqtQeaxam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "72530439-ff1e-4c38-b8e0-28b0d2c0844a"
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "batch = batch.to(dev)\n",
        "labels = labels.to(dev)\n",
        "scores, reconstructions = model(batch)\n",
        "print(scores.size())\n",
        "print(reconstructions.size())"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4, 3, 320, 320])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_iemIRmlwd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, dev, lr=0.001):\n",
        "    try:\n",
        "        # Create model\n",
        "        model = CapsNet(num_classes=num_classes)\n",
        "        model = model.to(dev)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        # Initialize history\n",
        "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        # Process each epoch\n",
        "        for epoch in range(epochs):\n",
        "            # Initialize epoch variables\n",
        "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "            \n",
        "            # Process each split\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                  model.train()\n",
        "                else:\n",
        "                  model.eval()\n",
        "                # Process each batch\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    # Move to CUDA\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    # Reset gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    # Compute output\n",
        "                    pred, reconstructions = model(input)\n",
        "                    score_loss = F.cross_entropy(pred, labels, weight=weights)\n",
        "                    reconstruction_loss = F.mse_loss(input, reconstructions)\n",
        "                    loss = score_loss + reconstruction_loss\n",
        "                    # Update loss\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    # Check parameter update\n",
        "                    if split == \"train\":\n",
        "                        # Compute gradients\n",
        "                        loss.backward()\n",
        "                        # Optimize\n",
        "                        optimizer.step()\n",
        "                    # Compute accuracy\n",
        "                    _,pred_labels = pred.max(1)\n",
        "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
        "                    # Update accuracy\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "            # Compute epoch loss/accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
        "            # Update history\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "            # Print info\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
        "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"VL={epoch_loss['val']:.4f},\",\n",
        "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
        "                  f\"TeA={epoch_accuracy['test']:.4f},\"\n",
        "                )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    finally:\n",
        "        # Plot loss\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # Plot accuracy\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcTyH7emXjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "9edd6d25-731f-4b05-bca7-71978e636845"
      },
      "source": [
        "train(100, dev, lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: TrL=2.1096, TrA=0.3154, VL=1.9799, VA=0.4337, TeL=1.9740, TeA=0.4301,\n",
            "Epoch 2: TrL=2.0044, TrA=0.4372, VL=1.9223, VA=0.4518, TeL=1.9202, TeA=0.4534,\n",
            "Epoch 3: TrL=1.9882, TrA=0.4436, VL=1.8991, VA=0.4639, TeL=1.9000, TeA=0.4407,\n",
            "Epoch 4: TrL=1.9831, TrA=0.4410, VL=1.8808, VA=0.4518, TeL=1.8798, TeA=0.4534,\n",
            "Epoch 5: TrL=1.9685, TrA=0.4474, VL=1.8437, VA=0.4518, TeL=1.8441, TeA=0.4534,\n",
            "Epoch 6: TrL=1.9492, TrA=0.4474, VL=1.8853, VA=0.4518, TeL=1.8745, TeA=0.4534,\n",
            "Epoch 7: TrL=1.9186, TrA=0.4487, VL=1.8116, VA=0.4518, TeL=1.8062, TeA=0.4534,\n",
            "Epoch 8: TrL=1.8869, TrA=0.4487, VL=1.8086, VA=0.4518, TeL=1.7944, TeA=0.4534,\n",
            "Epoch 9: TrL=1.8859, TrA=0.4423, VL=1.8275, VA=0.4518, TeL=1.8228, TeA=0.4534,\n",
            "Epoch 10: TrL=1.8696, TrA=0.4500, VL=1.7825, VA=0.4548, TeL=1.7735, TeA=0.4831,\n",
            "Epoch 11: TrL=1.8409, TrA=0.4295, VL=1.8144, VA=0.4066, TeL=1.8042, TeA=0.4682,\n",
            "Epoch 12: TrL=1.8206, TrA=0.4474, VL=1.7732, VA=0.4066, TeL=1.7748, TeA=0.4343,\n",
            "Epoch 13: TrL=1.7993, TrA=0.4321, VL=1.7958, VA=0.4458, TeL=1.7910, TeA=0.4725,\n",
            "Epoch 14: TrL=1.7709, TrA=0.4462, VL=1.7335, VA=0.4669, TeL=1.7232, TeA=0.4703,\n",
            "Epoch 15: TrL=1.7481, TrA=0.4615, VL=1.7333, VA=0.4548, TeL=1.7263, TeA=0.4703,\n",
            "Epoch 16: TrL=1.7381, TrA=0.4654, VL=1.7540, VA=0.4669, TeL=1.7498, TeA=0.4640,\n",
            "Epoch 17: TrL=1.7225, TrA=0.4782, VL=1.7093, VA=0.4699, TeL=1.6996, TeA=0.4746,\n",
            "Epoch 18: TrL=1.7161, TrA=0.4910, VL=1.7189, VA=0.4669, TeL=1.7125, TeA=0.4661,\n",
            "Epoch 19: TrL=1.6909, TrA=0.4782, VL=1.7119, VA=0.4759, TeL=1.7051, TeA=0.4979,\n",
            "Epoch 20: TrL=1.6726, TrA=0.4872, VL=1.6879, VA=0.4699, TeL=1.6810, TeA=0.4852,\n",
            "Epoch 21: TrL=1.6600, TrA=0.4782, VL=1.6857, VA=0.4699, TeL=1.6754, TeA=0.4746,\n",
            "Epoch 22: TrL=1.6520, TrA=0.4923, VL=1.6726, VA=0.4819, TeL=1.6628, TeA=0.5000,\n",
            "Epoch 23: TrL=1.6466, TrA=0.4897, VL=1.6734, VA=0.4669, TeL=1.6654, TeA=0.5042,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}